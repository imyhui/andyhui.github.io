<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[糗事百科爬虫]]></title>
    <url>%2F2017%2F09%2F02%2Fqsbkspider%2F</url>
    <content type="text"><![CDATA[一周前买了阿里云服务器，简单部署了一个JudgeService，感觉闲着也是闲着，决定在上面部署一个爬虫，打算每隔一段时间爬取糗事百科前几页的文本段子并以邮件的形式发送到qq邮箱中。 基本环境 requests + smtplib + bs4都可以用pip install 来安装 分析需求 基本目的是爬取糗事百科文本部分前几页内容保存后续操作有通过邮件发送到邮箱，之后是挂载到云服务器上每隔一段时间自动爬取并发送邮件 分析url 我们这次只爬取文字内容，所以这次爬取的url是 ‘https://www.qiushibaike.com/text/‘点开第二页会发现 url变为 ‘https://www.qiushibaike.com/text/page/2/‘很清晰的知道 第i页的url也就是 url = ‘https://www.qiushibaike.com/text/page/%s/&#39;%str(i)我们要爬前多少页也就是一个for循环的事 分析网页源码 首先来看下网页的基本内容 我们要做的是提取这一个个文本，然后保存下来 根据网页源码很容易看出 内容是在 class=”content”的div标签下，可以直接套用正则表达式，我们这使用BeautifulSoup库的find_all函数就可以搞定 构造request请求 首先就是通过requests库得到网页源码 html = requests.get(url) 这里我们加一个小的异常处理，也就是如果爬取不到我们将错误信息写入一个文件，文件名为Http error on time.ctime() 这里的time.ctime()是包含在time里面的一个函数，返回当前时间。 然后用BeautifulSoup做成一锅汤soup = BeautifulSoup(html.text, ‘lxml’)这里我们用lxml HTML 解析器，因为它的优势是速度快，文档容错能力强，(更多关于BeautifulSoup)[http://cuiqingcai.com/1319.html] 之后我们用_findall找到每一个笑话，之后呢，把换行标签替换掉，然后加到data_list 中去 123456789101112131415161718192021222324252627282930import requestsfrom bs4 import BeautifulSoupimport timeimport lxmldef getcontent(url): try: html = requests.get(url) except: with open("log.log","a") as file: file.write("Http error on " + time.ctime()) time.sleep(60) return None soup = BeautifulSoup(html.text, 'lxml') data_list = [] for cont in soup.find_all("div", &#123;"class":"content"&#125;): raw_data = cont.get_text() data = raw_data.replace("\n","") data_list.append(data) return data_listdef main(): data_list = [] for i in range(1,2): url = 'https://www.qiushibaike.com/text/page/%s/'%str(i) temp_data = getcontent(url) data_list.extend(temp_data) for i in data_list: print(i) print('\n\n')main() 未完待续]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
        <tag>requests</tag>
        <tag>linux</tag>
        <tag>BeautifulSoup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[oj信息爬取]]></title>
    <url>%2F2017%2F08%2F29%2Fojrankscan%2F</url>
    <content type="text"><![CDATA[假期俱乐部举办了编程训练营，每个人负责管理15人的营，每天作业会在oj的一个总榜上关于各营营长每天统计很麻烦，所以我写了一个简单的爬虫来节省一部分工作 代码改变世界，使人更高效的完成自己的工作 基本环境 Windows 10 + python 3.6.2 + requests 库 requests 库安装1pip install requests 分析需求 首先需要统计的 Contest 有3周的作业 加 最后的结课测试 每个榜单结构都是一致的，我只需要统计自己营里的昵称和总解决数目就好 url 是 “http://oj.acmclub.cn/contestrank.php?cid=“ + contestID 右键查看网页源代码 两个a标签中刚好有我们的数据，用简单正则表达式匹配下就好 构造request爬取网页 首先拿出一个榜单来处理，由于不需要登陆就可以查看榜单所以我直接抓取榜单html页面 12345678910111213141516import requestsdef getHTMLText(url): try: r = requests.get(url, timeout=30) r.raise_for_status() r.encoding = r.apparent_encoding return r.text except: return ""def main(): for no in range(1166,1169): url = 'http://oj.acmclub.cn/contestrank.php?cid='+str(no) html = getHTMLText(url) print(html)main() 这算一个基本框架了，通过request得到网页源码，中间_r.raise_forstatus()是错误检查，后面是根据推断的编码类型设置字符编码运行结果如下 对html源码处理 首先看网页源码 这里可以用正则表达式库 re 来进行字符匹配，如果昵称符合规范xx营xx号_Nickname_name就很容易处理了，匹配两个a标签之间的内容1rege = r'&lt;a href=.*?&gt;(0&#123;0,1&#125;'+str(num)+'营.*?)&lt;/a&gt;&lt;td&gt;&lt;a href=.*?&gt;([0-9]&#123;1,2&#125;)&lt;/a&gt;' 对html的处理函数也就是很容易写了 1234567891011121314151617181920import requestsimport redef getHTMLText(url): #省略def fillscoreList(slist, html, num): rege = r'&lt;a href=.*?&gt;(0&#123;0,1&#125;'+str(num)+'营.*?)&lt;/a&gt;&lt;td&gt;&lt;a href=.*?&gt;([0-9]&#123;1,2&#125;)&lt;/a&gt;' score = re.findall(rege,html) for x in score: slist.append(x)def main(): num = int(input("请输入营号:")) for no in range(1166,1169): sinfo = [] url = 'http://oj.acmclub.cn/contestrank.php?cid='+str(no) html = getHTMLText(url) fillscoreList(sinfo, html, num) for (name,solve) in sinfo: print(name,solve)main() 这样得到的sinfo就是包含元组(name,solve)的列表，程序到这阶段基本算是完工了，但是输出的样式也并不尽人意，比如没有对齐，看起来很乱，而且三周内容挤在一块不好区分，接下来就对这个程序进行优化 格式化输出 我们想要达到的效果是三周内容清晰可辨，并且有良好的对齐，下面就来是实现下python 的 字符串 有format函数，通过这个来达到我们想要的效果对于(xx营xx号_Nickname_name,solve_num)这样一个元组，通过格式限定符来达到指定字段宽度和居中对齐 12tplt = "&#123;0:&lt;20&#125;\t\t&#123;1:^3&#125;"print(tplt.format("xx营xx号_Nickname_name"," 解决总题目数",chr(12288))) {}来指明位置 相当于c的printf中的%，{0} 指的是第0个元素，填充常跟对齐一起使用^、&lt;、&gt;分别是居中、左对齐、右对齐，后面带宽度:号后面带填充的字符，只能是一个字符，不指定的话默认是用空格填充，后面我们指定了中文空格 所以现在的程序就是这样 12345678910111213141516171819202122import requestsimport redef getHTMLText(url): #省略def fillscoreList(slist, html, num): #省略def printscoreList(slist, num): tplt = "&#123;0:20&#125;\t\t&#123;1:^3&#125;" print(tplt.format("xx营xx号_Nickname_name"," 解决题目数",chr(12288))) for i in range(num): u=slist[i] print(tplt.format(u[0],u[1],chr(12288)))def main(): num = int(input("请输入营号:")) for no in range(1166,1169): sinfo = [] url = 'http://oj.acmclub.cn/contestrank.php?cid='+str(no) html = getHTMLText(url) fillscoreList(sinfo, html, num) print("*"*15,"第%d周%d营成绩"%(int(no-1165),num),"*"*18) printscoreList(sinfo, len(sinfo))main() 运行结果如下 未完待续]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>oj</tag>
        <tag>python</tag>
        <tag>requests</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown 基本语法]]></title>
    <url>%2F2017%2F08%2F25%2FMarkdown_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[近几天刚刚搭建了博客，用的是Hexo+Next主题，托管在github和codding上，写博文是需要Markdown，所以先学习下Markdown的基本语法，也算是为博客增加一篇博文吧。 Markdown基础用法与规则： 标题使用”#”加空格在首行来创建标题如:&emsp;&emsp; # 一级标题&emsp;&emsp; ## 二级标题&emsp;&emsp; ### 三级标题 加粗功能使用一组星号”**“或一组下划线”__“来加粗一段文字，用转义符”\“来打出”*“如:&emsp;&emsp; 这是加粗的文字&emsp;&emsp; 这也是加粗的文字 引用使用”&gt;”在段首来引用一段文字，要在引用前后加入空白行声明开始和结束引用如: 这是一段引用这是一段引用 无序列表使用”-“、”*”或”+”加空格来创建无序列表如: 这是一个无序列表 这是一个无序列表 这是一个无序列表 有序列表使用数字圆点加空格如”1.”、”2.”来创建有序列表如: 这是一个有序列表 这是一个有序列表 这是一个有序列表 以上来源锤子便签 贴代码用一对重音符”``` code ```“引起来，可以在```后表明语言如: 1234567#include&lt;iostream&gt;using namespace std;int main()&#123; cout&lt;&lt;"Hello World!"&lt;&lt;endl; return 0;&#125; 也可以用4个空格(Tab)缩进再贴上代码实现相同的效果 #include&lt;iostream&gt; using namespace std; int main() { cout&lt;&lt;&quot;Hello World!&quot;&lt;&lt;endl; return 0; } 强调标记用两个重音符”`强调内容`“这是一个强调标记 未完待续]]></content>
      <categories>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F08%2F24%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Welcome to my blog]]></title>
    <url>%2F2017%2F08%2F23%2FWelcome%2F</url>
    <content type="text"><![CDATA[123456//C#include&lt;stdio.h&gt;int main(void)&#123; printf("Hello,my friend\n"); return 0;&#125; 12345678//c++#include&lt;iostream&gt;using namespace std;int main()&#123; cout&lt;&lt;"Welcome to andyhui's blog!"&lt;&lt;endl; return 0;&#125; 123456//Javapublic class Main &#123; public static void main(String[] args)&#123; System.out.println("There are some blog posts here."); &#125;&#125; 12#Pythonprint("Hope it helps you!")]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>code</tag>
        <tag>C++</tag>
      </tags>
  </entry>
</search>
